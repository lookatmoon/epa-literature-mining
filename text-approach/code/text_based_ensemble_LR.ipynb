{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bobX8Zd7Zpl0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import Ridge,SGDRegressor,LinearRegression\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import matplotlib as mpl \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfv_covikoCh"
      },
      "source": [
        "#import the training sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-3AwSkduL4b"
      },
      "outputs": [],
      "source": [
        "#import resulted training sets\n",
        "c3_train = pd.read_csv('./c3.csv',engine='python',encoding = 'unicode_escape')\n",
        "c4_train = pd.read_csv('./c4.csv',engine='python',encoding = 'unicode_escape')\n",
        "c5_train = pd.read_csv('./c5.csv',engine='python',encoding = 'unicode_escape')\n",
        "c6_train = pd.read_csv('./c6.csv',engine='python',encoding = 'unicode_escape')\n",
        "c7_train = pd.read_csv('./c7.csv',engine='python',encoding = 'unicode_escape')\n",
        "c8_train = pd.read_csv('./c8.csv',engine='python',encoding = 'unicode_escape')\n",
        "c9_train = pd.read_csv('./c9.csv',engine='python',encoding = 'unicode_escape')\n",
        "c10_train = pd.read_csv('./c10.csv',engine='python',encoding = 'unicode_escape')\n",
        "c10_train = pd.read_csv('./c10_train_cleaned.csv',engine='python',encoding = 'unicode_escape')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZx1cqhVS-GY"
      },
      "outputs": [],
      "source": [
        "c32_train = pd.read_csv('./c3.2.csv',engine='python',encoding = 'unicode_escape')\n",
        "c33_train = pd.read_csv('./c3.3-3.8.csv',engine='python',encoding = 'unicode_escape')\n",
        "c61_train = pd.read_csv('./c6.1-6.6.csv',engine='python',encoding = 'unicode_escape')\n",
        "c63_train = pd.read_csv('./c6.3.csv',engine='python',encoding = 'unicode_escape')\n",
        "c71_train = pd.read_csv('./c7.1-7.7.csv',engine='python',encoding = 'unicode_escape')\n",
        "c91_train = pd.read_csv('./c9.1-9.6.csv',engine='python',encoding = 'unicode_escape')\n",
        "c94_train = pd.read_csv('./c9.4.csv',engine='python',encoding = 'unicode_escape')\n",
        "c95_train = pd.read_csv('./c9.5.csv',engine='python',encoding = 'unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am7Ww7EzC03L"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('./s2020.csv',encoding='utf-8',engine = 'python', error_bad_lines=False)\n",
        "# test = pd.read_csv('./test.csv',engine='python',encoding='utf-8',error_bad_lines=False,index_col=False,names = ['InstanceId','HeroId','Part','Chapter','Section','Subsection','AnchorText','ContextParagraph','PMID','Title','Abstract','Cited'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Poqd7Q9UnXYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a953a1-a1b1-4cc5-b784-94b9d082ba7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171376"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Blekk1OrspX"
      },
      "source": [
        "#construct features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSFMqybRFLGp",
        "outputId": "a34c400c-42ed-4bba-8ff0-79dd4ae6646c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words()\n",
        "\n",
        "def cleaning(text):\n",
        "    \"\"\"\n",
        "    Convert to lowercase.\n",
        "    Rremove URL links, special characters and punctuation.\n",
        "    Tokenize and remove stop words.\n",
        "    \"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('[’“”…]', '', text)\n",
        "\n",
        "    # removing the stop-words\n",
        "    text_tokens = word_tokenize(text)\n",
        "    tokens_without_sw = [\n",
        "        word for word in text_tokens if not word in STOP_WORDS]\n",
        "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "    text = filtered_sentence\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title_and_abstract(df):\n",
        "    corpus = []\n",
        "    for index, row in df.iterrows():\n",
        "        title = row['TITLE']\n",
        "        abstract = row['ABSTRACT']\n",
        "        corpus.append( str(title) + ' ' + str(abstract))\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "oAfEMofSSzh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer3 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c3_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer3_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c3_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer4 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c4_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer4_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c4_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer5 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c5_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer5_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c5_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer6 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c6_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer6_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c6_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer7 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c7_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer7_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c7_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer8 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c8_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer8_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c8_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer9 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c9_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer9_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c9_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer10 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c10_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer10_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c10_train['TITLE'].apply(cleaning).values.astype('U'))"
      ],
      "metadata": {
        "id": "mamE123DjLFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer32 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c32_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer32_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c32_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer33 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c33_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer33_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c33_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer61 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c61_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer61_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c61_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer63 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c63_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer63_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c63_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer71 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c71_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer71_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c71_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer91 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c91_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer91_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c91_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer94 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c94_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer94_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c94_train['TITLE'].apply(cleaning).values.astype('U'))\n",
        "vectorizer95 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c95_train['ABSTRACT'].apply(cleaning).values.astype('U'))\n",
        "vectorizer95_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c95_train['TITLE'].apply(cleaning).values.astype('U'))"
      ],
      "metadata": {
        "id": "v-wUFk0tjGny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JCQqXBAvHA_a"
      },
      "outputs": [],
      "source": [
        "# construct sparse feature matrix\n",
        "# params:\n",
        "#     df: dataframe, with 'ABSTRACT' and 'TITLE' columns\n",
        "#     vectorizer: sklearn text vectorizer, either TfidfVectorizer or Countvectorizer \n",
        "# return:\n",
        "#     M: a sparse feature matrix that represents df's textual information (used by a predictive model)\n",
        "\n",
        "def construct_feature_matrix(df, vectorizer, vectorizer2):\n",
        "    abstract = df['ABSTRACT'].apply(lambda x: np.str_(x)).tolist()\n",
        "    title = df['TITLE'].apply(lambda x: np.str_(x)).tolist()\n",
        "  \n",
        "    # here the dimensionality of X is len(df) x |V|\n",
        "    X = vectorizer.transform(abstract)\n",
        "    Y = vectorizer2.transform(title)\n",
        "    Z = scipy.sparse.hstack([X,Y])\n",
        "\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1t0YH6eImMm",
        "outputId": "c3308e85-73b0-4aa5-886f-6eb8237ed5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15772, 2209728)\n",
            "(171376, 2209728)\n"
          ]
        }
      ],
      "source": [
        "train_Y_c3 = c3_train['CITED']\n",
        "train_X_c3 = construct_feature_matrix(c3_train, vectorizer3, vectorizer3_2)\n",
        "test_X_c3 = construct_feature_matrix(test, vectorizer3, vectorizer3_2)\n",
        "print(train_X_c3.shape)\n",
        "print(test_X_c3.shape)\n",
        "train_Y_c4 = c4_train['CITED']\n",
        "train_X_c4 = construct_feature_matrix(c4_train, vectorizer4, vectorizer4_2)\n",
        "test_X_c4 = construct_feature_matrix(test, vectorizer4, vectorizer4_2)\n",
        "\n",
        "train_Y_c5 = c5_train['CITED']\n",
        "train_X_c5 = construct_feature_matrix(c5_train, vectorizer5, vectorizer5_2)\n",
        "test_X_c5 = construct_feature_matrix(test, vectorizer5, vectorizer5_2)\n",
        "\n",
        "train_Y_c6 = c6_train['CITED']\n",
        "train_X_c6 = construct_feature_matrix(c6_train, vectorizer6, vectorizer6_2)\n",
        "test_X_c6 = construct_feature_matrix(test, vectorizer6, vectorizer6_2)\n",
        "\n",
        "train_Y_c7 = c7_train['CITED']\n",
        "train_X_c7 = construct_feature_matrix(c7_train, vectorizer7, vectorizer7_2)\n",
        "test_X_c7 = construct_feature_matrix(test, vectorizer7, vectorizer7_2)\n",
        "\n",
        "train_Y_c8 = c8_train['CITED']\n",
        "train_X_c8 = construct_feature_matrix(c8_train, vectorizer8, vectorizer8_2)\n",
        "test_X_c8 = construct_feature_matrix(test, vectorizer8, vectorizer8_2)\n",
        "\n",
        "train_Y_c9 = c9_train['CITED']\n",
        "train_X_c9 = construct_feature_matrix(c9_train, vectorizer9, vectorizer9_2)\n",
        "test_X_c9 = construct_feature_matrix(test, vectorizer9, vectorizer9_2)\n",
        "\n",
        "train_Y_c10 = c10_train['CITED']\n",
        "train_X_c10 = construct_feature_matrix(c10_train, vectorizer10, vectorizer10_2)\n",
        "test_X_c10 = construct_feature_matrix(test, vectorizer10, vectorizer10_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y_c32 = c32_train['CITED']\n",
        "train_X_c32 = construct_feature_matrix(c32_train, vectorizer32, vectorizer32_2)\n",
        "test_X_c32 = construct_feature_matrix(test, vectorizer32, vectorizer32_2)\n",
        "\n",
        "train_Y_c33 = c33_train['CITED']\n",
        "train_X_c33 = construct_feature_matrix(c33_train, vectorizer33, vectorizer33_2)\n",
        "test_X_c33 = construct_feature_matrix(test, vectorizer33, vectorizer33_2)\n",
        "\n",
        "train_Y_c61 = c61_train['CITED']\n",
        "train_X_c61 = construct_feature_matrix(c61_train, vectorizer61, vectorizer61_2)\n",
        "test_X_c61 = construct_feature_matrix(test, vectorizer61, vectorizer61_2)\n",
        "\n",
        "train_Y_c63 = c63_train['CITED']\n",
        "train_X_c63 = construct_feature_matrix(c63_train, vectorizer63, vectorizer63_2)\n",
        "test_X_c63 = construct_feature_matrix(test, vectorizer63, vectorizer63_2)\n",
        "\n",
        "train_Y_c71 = c71_train['CITED']\n",
        "train_X_c71 = construct_feature_matrix(c71_train, vectorizer71, vectorizer71_2)\n",
        "test_X_c71 = construct_feature_matrix(test, vectorizer71, vectorizer71_2)\n",
        "\n",
        "train_Y_c91 = c91_train['CITED']\n",
        "train_X_c91 = construct_feature_matrix(c91_train, vectorizer91, vectorizer91_2)\n",
        "test_X_c91 = construct_feature_matrix(test, vectorizer91, vectorizer91_2)\n",
        "\n",
        "train_Y_c94 = c94_train['CITED']\n",
        "train_X_c94 = construct_feature_matrix(c94_train, vectorizer94, vectorizer94_2)\n",
        "test_X_c94 = construct_feature_matrix(test, vectorizer94, vectorizer94_2)\n",
        "\n",
        "train_Y_c95 = c95_train['CITED']\n",
        "train_X_c95 = construct_feature_matrix(c95_train, vectorizer95, vectorizer95_2)\n",
        "test_X_c95 = construct_feature_matrix(test, vectorizer95, vectorizer95_2)"
      ],
      "metadata": {
        "id": "uatVli2yYfkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZeGxcRtrwom"
      },
      "source": [
        "#create helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArXT-j10o691"
      },
      "outputs": [],
      "source": [
        "def merge(list1, list2, list3, list4):\n",
        "    merged_list = []\n",
        "    for i in range(max((len(list1), len(list2),len(list3),len(list4)))):\n",
        "        while True:\n",
        "            try:\n",
        "                tup = (list1[i], list2[i], list3[i],list4[i])\n",
        "            except IndexError:\n",
        "                if len(list3) > len(list2):\n",
        "                    list2.append('')\n",
        "                    tup = (list1[i], list2[i],list3[i],list4[i])\n",
        "                elif len(list3) < len(list2):\n",
        "                    list3.append('')\n",
        "                    tup = (list1[i], list2[i],list3[i],list4[i])\n",
        "                continue\n",
        "            merged_list.append(tup)\n",
        "            break\n",
        "    return merged_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RnusBWgro6C"
      },
      "outputs": [],
      "source": [
        "def calculate_recall(filepath,pmid_only,num):\n",
        "  #filepath is a list\n",
        "  pair = []\n",
        "  filepath.sort(key=lambda y: y[3],reverse=True)\n",
        "  if pmid_only == True:\n",
        "    filepath=list(filter(lambda c: np.isnan(c[1]) == False, filepath))\n",
        "  for i in range(1,len(filepath),num):\n",
        "    tp = 0\n",
        "    fn = 0\n",
        "    for j in range(1,i):\n",
        "      if filepath[j][2] == 1:\n",
        "        tp += 1\n",
        "    for k in range(i+1,len(filepath)):\n",
        "      if filepath[k][2] == 1:\n",
        "        fn += 1\n",
        "    pair.append((i,tp/(tp+fn)))\n",
        "  return pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czG1IPlPrprL"
      },
      "outputs": [],
      "source": [
        "# def calculate_precision(filepath,pmid_only,num):\n",
        "#   #filepath is a list\n",
        "#   pair = []\n",
        "#   if pmid_only == True:\n",
        "#     filepath=list(filter(lambda c: np.isnan(c[1]) == False, filepath))\n",
        "#   for i in range(1,len(filepath),num):\n",
        "#     tp = 0\n",
        "#     for j in range(1,i):\n",
        "#       if filepath[j][2] == 1:\n",
        "#         tp += 1\n",
        "#     pair.append((i,tp))\n",
        "#   result = []\n",
        "#   result.append((0,0))\n",
        "#   for i in range(0,len(pair)-1):\n",
        "#     result.append((pair[i][0],(pair[i+1][1]-pair[i][1])/num))\n",
        "  \n",
        "#     # pair.append((i,tp/(i)))\n",
        "#   return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU3zkETEr0Tc"
      },
      "source": [
        "#start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p38UdSEAo7q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0311de-ecc4-4422-d430-7e7d60631f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1936515, nan, 0, 0.99861856338975), (90753, nan, 0, 0.990415896812988), (974956, nan, 0, 0.9889023809858714), (3227028, 32742280.0, 0, 0.9866207209377155), (3010751, 26029963.0, 0, 0.9860535126691515), (2379065, nan, 0, 0.985536544925395), (1559953, nan, 0, 0.9849364278115993), (90352, nan, 0, 0.9839364745717085), (190195, 16570610.0, 0, 0.9825597335782789), (381127, nan, 0, 0.9819377031042251)]\n"
          ]
        }
      ],
      "source": [
        "#C3\n",
        "model_c3 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c3.fit(train_X_c3, train_Y_c3)\n",
        "test_Y_hat_c3 = model_c3.predict_proba(test_X_c3)\n",
        "\n",
        "prob_c3=[]\n",
        "for i in range(0,len(test_Y_hat_c3)):\n",
        "  prob_c3.append(test_Y_hat_c3[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c3 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c3)\n",
        "merged_list_c3.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c3[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MDf8uZgSsoY",
        "outputId": "451b2a8f-dfcb-410e-e5a0-7c8289fa6fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2099076, nan, 0, 0.9973626962409099), (3846692, 30147852.0, 1, 0.9972332198596723), (749707, 21545154.0, 0, 0.9957934382711614), (1523683, nan, 0, 0.9953934066436473), (1724119, nan, 0, 0.9952699998648498), (90560, nan, 1, 0.9939848337486482), (1063158, nan, 1, 0.993075546990921), (1721880, nan, 1, 0.9916569299168823), (3068244, nan, 0, 0.9915973620730337), (3846538, 30147851.0, 0, 0.9912356983793728)]\n"
          ]
        }
      ],
      "source": [
        "#C4\n",
        "model_c4 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c4.fit(train_X_c4, train_Y_c4)\n",
        "test_Y_hat_c4 = model_c4.predict_proba(test_X_c4)\n",
        "\n",
        "prob_c4=[]\n",
        "for i in range(0,len(test_Y_hat_c4)):\n",
        "  prob_c4.append(test_Y_hat_c4[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c4 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c4)\n",
        "merged_list_c4.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c4[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgT-ZI0oTtZx",
        "outputId": "928f5ac8-9386-48f0-cf73-92679b8b4977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2353727, 24857953.0, 0, 0.9954310677376339), (192003, 19440509.0, 0, 0.9938334467314407), (40308, 4051323.0, 0, 0.9916093242760605), (85807, 9546760.0, 0, 0.9899789923455574), (78485, 7582284.0, 0, 0.9878780585807221), (4330573, 22641258.0, 0, 0.9874782822120804), (757782, 21530391.0, 0, 0.9873291990367951), (196494, 16556581.0, 0, 0.9859909692936061), (79203, 8820584.0, 0, 0.9856436092747957), (93690, 12122573.0, 1, 0.9856340821213269)]\n"
          ]
        }
      ],
      "source": [
        "#C5\n",
        "model_c5 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c5.fit(train_X_c5, train_Y_c5)\n",
        "test_Y_hat_c5 = model_c5.predict_proba(test_X_c5)\n",
        "\n",
        "prob_c5=[]\n",
        "for i in range(0,len(test_Y_hat_c5)):\n",
        "  prob_c5.append(test_Y_hat_c5[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c5 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c5)\n",
        "merged_list_c5.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c5[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqzopNId9Vf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5b9989-9fa6-4d91-8066-ffff789e91be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2826788, 25674828.0, 0, 0.9982628172124638), (1931111, 23876070.0, 0, 0.9967963411900249), (80086, 15721891.0, 0, 0.9966848534968035), (4245806, 29147121.0, 0, 0.9966413383033313), (3860965, 28735838.0, 0, 0.9964320760190858), (597250, 20064780.0, 0, 0.9962638851998772), (3007799, 26408041.0, 0, 0.9960910722350543), (3008722, 26276052.0, 0, 0.9958614336534847), (2331765, 23838152.0, 1, 0.9948612151234952), (192076, 18786668.0, 0, 0.9947786095385661)]\n"
          ]
        }
      ],
      "source": [
        "#C6\n",
        "model_c6 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c6.fit(train_X_c6, train_Y_c6)\n",
        "test_Y_hat_c6 = model_c6.predict_proba(test_X_c6)\n",
        "\n",
        "prob_c6=[]\n",
        "for i in range(0,len(test_Y_hat_c6)):\n",
        "  prob_c6.append(test_Y_hat_c6[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c6 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c6)\n",
        "merged_list_c6.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c6[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw0R2ky49hLO",
        "outputId": "b5da7c8a-d659-485c-f31d-7a868172163b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(55613, 14684720.0, 0, 0.9960052350744097), (44274, 7619333.0, 0, 0.9938979577237395), (3159535, 26851724.0, 0, 0.9923750554331792), (30111, 9697229.0, 0, 0.9923280783134358), (4652814, 27436694.0, 0, 0.9912348859737343), (7028, 10464077.0, 0, 0.9907090176616109), (670076, 20667084.0, 0, 0.9891225653533268), (89818, 16907939.0, 0, 0.988750103475659), (1258310, 22726801.0, 0, 0.9881510192025937), (87895, 16046604.0, 0, 0.9862430485338968)]\n"
          ]
        }
      ],
      "source": [
        "#C7\n",
        "model_c7 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c7.fit(train_X_c7, train_Y_c7)\n",
        "test_Y_hat_c7 = model_c7.predict_proba(test_X_c7)\n",
        "\n",
        "prob_c7=[]\n",
        "for i in range(0,len(test_Y_hat_c7)):\n",
        "  prob_c7.append(test_Y_hat_c7[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c7 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c7)\n",
        "merged_list_c7.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c7[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaR1nR5Z9rpg",
        "outputId": "e68d205c-ca72-4864-cba9-8d0c848800df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(3299814, nan, 0, 0.9959187592479405), (4245384, 29393991.0, 0, 0.9939895036803856), (191221, 17080957.0, 0, 0.9867690761615476), (38348, nan, 1, 0.9863324604780318), (180341, nan, 0, 0.980676859054658), (4347011, 22710144.0, 0, 0.9788368534264867), (625703, nan, 0, 0.9775208647762939), (4303784, 25703019.0, 0, 0.9763773455935805), (191533, 19054359.0, 0, 0.9738227573187423), (191425, 16913859.0, 0, 0.9730550314391447)]\n"
          ]
        }
      ],
      "source": [
        "#C8\n",
        "model_c8 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c8.fit(train_X_c8, train_Y_c8)\n",
        "test_Y_hat_c8 = model_c8.predict_proba(test_X_c8)\n",
        "\n",
        "prob_c8=[]\n",
        "for i in range(0,len(test_Y_hat_c8)):\n",
        "  prob_c8.append(test_Y_hat_c8[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c8 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c8)\n",
        "merged_list_c8.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c8[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sI0Bkg_93K7",
        "outputId": "c9d2e166-036d-4501-93dd-283273df2cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2495717, 23504696.0, 0, 0.9971448552645293), (191341, nan, 0, 0.9927529991757692), (3411422, nan, 0, 0.9916183966483593), (3162581, 26546275.0, 0, 0.9914466756297434), (4305448, nan, 0, 0.9908626546563043), (3296838, nan, 0, 0.9901757374855763), (191345, 16137936.0, 0, 0.9898676667451721), (191470, nan, 0, 0.989777437220708), (2489575, nan, 0, 0.9896746633992496), (3294474, nan, 0, 0.9886302179237072)]\n"
          ]
        }
      ],
      "source": [
        "#C9\n",
        "model_c9 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c9.fit(train_X_c9, train_Y_c9)\n",
        "test_Y_hat_c9 = model_c9.predict_proba(test_X_c9)\n",
        "\n",
        "prob_c9=[]\n",
        "for i in range(0,len(test_Y_hat_c9)):\n",
        "  prob_c9.append(test_Y_hat_c9[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c9 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c9)\n",
        "merged_list_c9.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c9[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t-utbHX-FfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5c8232-e388-4a4a-9580-fa0a18a7f1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(42136, 15092540.0, 1, 0.9890853125963566), (121678, nan, 0, 0.9877745598660174), (40706, nan, 1, 0.9869072721115352), (55350, nan, 0, 0.9856674171598827), (40451, 26355372.0, 1, 0.9843355411683674), (42137, nan, 1, 0.9835149284383852), (26664, nan, 1, 0.9833891957606514), (42537, 15092872.0, 1, 0.9832714388486954), (30380, 15093382.0, 0, 0.9820722007546052), (42545, nan, 0, 0.9819533459928943)]\n"
          ]
        }
      ],
      "source": [
        "#C10\n",
        "model_c10 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c10.fit(train_X_c10, train_Y_c10)\n",
        "test_Y_hat_c10 = model_c10.predict_proba(test_X_c10)\n",
        "\n",
        "prob_c10=[]\n",
        "for i in range(0,len(test_Y_hat_c10)):\n",
        "  prob_c10.append(test_Y_hat_c10[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c10 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c10)\n",
        "merged_list_c10.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c10[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#C3.2\n",
        "model_c32 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c32.fit(train_X_c32, train_Y_c32)\n",
        "test_Y_hat_c32 = model_c32.predict_proba(test_X_c32)\n",
        "\n",
        "prob_c32=[]\n",
        "for i in range(0,len(test_Y_hat_c32)):\n",
        "  prob_c32.append(test_Y_hat_c32[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c32 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c32)\n",
        "merged_list_c32.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c32[0:10])"
      ],
      "metadata": {
        "id": "fG5PK-XdZ_3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C3.3\n",
        "model_c33 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c33.fit(train_X_c33, train_Y_c33)\n",
        "test_Y_hat_c33 = model_c33.predict_proba(test_X_c33)\n",
        "\n",
        "prob_c33=[]\n",
        "for i in range(0,len(test_Y_hat_c33)):\n",
        "  prob_c33.append(test_Y_hat_c33[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c33 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c33)\n",
        "merged_list_c33.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c33[0:10])"
      ],
      "metadata": {
        "id": "j8FGcfXZXMag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C6.1\n",
        "model_c61 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c61.fit(train_X_c61, train_Y_c61)\n",
        "test_Y_hat_c61 = model_c61.predict_proba(test_X_c61)\n",
        "\n",
        "prob_c61=[]\n",
        "for i in range(0,len(test_Y_hat_c61)):\n",
        "  prob_c61.append(test_Y_hat_c61[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c61 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c61)\n",
        "merged_list_c61.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c61[0:10])"
      ],
      "metadata": {
        "id": "gxUfaffCY9dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C6.3\n",
        "model_c63 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c63.fit(train_X_c63, train_Y_c63)\n",
        "test_Y_hat_c63 = model_c63.predict_proba(test_X_c63)\n",
        "\n",
        "prob_c63=[]\n",
        "for i in range(0,len(test_Y_hat_c63)):\n",
        "  prob_c63.append(test_Y_hat_c63[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c63 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c63)\n",
        "merged_list_c63.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c63[0:10])"
      ],
      "metadata": {
        "id": "Xf89MwVaZPQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C7.1\n",
        "model_c71 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c71.fit(train_X_c71, train_Y_c71)\n",
        "test_Y_hat_c71 = model_c71.predict_proba(test_X_c71)\n",
        "\n",
        "prob_c71=[]\n",
        "for i in range(0,len(test_Y_hat_c71)):\n",
        "  prob_c71.append(test_Y_hat_c71[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c71 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c71)\n",
        "merged_list_c71.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c71[0:10])"
      ],
      "metadata": {
        "id": "KRK8twgaa3Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C9.1\n",
        "model_c91 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c91.fit(train_X_c91, train_Y_c91)\n",
        "test_Y_hat_c91 = model_c91.predict_proba(test_X_c91)\n",
        "\n",
        "prob_c91=[]\n",
        "for i in range(0,len(test_Y_hat_c91)):\n",
        "  prob_c91.append(test_Y_hat_c91[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c91 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c91)\n",
        "merged_list_c91.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c91[0:10])"
      ],
      "metadata": {
        "id": "Fpm_ui60bngw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C9.4\n",
        "model_c94 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c94.fit(train_X_c94, train_Y_c94)\n",
        "test_Y_hat_c94 = model_c94.predict_proba(test_X_c94)\n",
        "\n",
        "prob_c94=[]\n",
        "for i in range(0,len(test_Y_hat_c94)):\n",
        "  prob_c94.append(test_Y_hat_c94[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c94 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c94)\n",
        "merged_list_c94.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c94[0:10])"
      ],
      "metadata": {
        "id": "oMusUq-ebxIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#C9.5\n",
        "model_c95 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')\n",
        "model_c95.fit(train_X_c95, train_Y_c95)\n",
        "test_Y_hat_c95 = model_c95.predict_proba(test_X_c95)\n",
        "\n",
        "prob_c95=[]\n",
        "for i in range(0,len(test_Y_hat_c95)):\n",
        "  prob_c95.append(test_Y_hat_c95[i][1])\n",
        "\n",
        "#create list of tuples and sort\n",
        "merged_list_c95 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c95)\n",
        "merged_list_c95.sort(key=lambda y: y[3],reverse=True)\n",
        "print(merged_list_c95[0:10])"
      ],
      "metadata": {
        "id": "CybH1GU4cCW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Kjek4l3s-w"
      },
      "outputs": [],
      "source": [
        "merged_list_c3.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c4.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c5.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c6.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c7.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c8.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c9.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c10.sort(key=lambda y: y[0],reverse=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_list_c32.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c33.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c61.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c63.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c71.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c91.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c94.sort(key=lambda y: y[0],reverse=False)\n",
        "merged_list_c95.sort(key=lambda y: y[0],reverse=False)"
      ],
      "metadata": {
        "id": "ZlEitZ8BWjgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrC-jr7U3S3m"
      },
      "outputs": [],
      "source": [
        "hero = []\n",
        "for i in range(0,len(merged_list_c3)):\n",
        "  hero.append(merged_list_c4[i][0])\n",
        "\n",
        "pmid = []\n",
        "for i in range(0,len(merged_list_c3)):\n",
        "  pmid.append(merged_list_c4[i][1])\n",
        "cited = []\n",
        "for i in range(0,len(merged_list_c3)):\n",
        "  cited.append(merged_list_c4[i][2])\n",
        "\n",
        "score_c3 = []\n",
        "for i in range(0,len(merged_list_c3)):\n",
        "  score_c3.append(merged_list_c3[i][3])\n",
        "\n",
        "score_c4 = []\n",
        "for i in range(0,len(merged_list_c4)):\n",
        "  score_c4.append(merged_list_c4[i][3])\n",
        "\n",
        "score_c5 = []\n",
        "for i in range(0,len(merged_list_c5)):\n",
        "  score_c5.append(merged_list_c5[i][3])\n",
        "\n",
        "score_c6 = []\n",
        "for i in range(0,len(merged_list_c6)):\n",
        "  score_c6.append(merged_list_c6[i][3])\n",
        "\n",
        "score_c7 = []\n",
        "for i in range(0,len(merged_list_c7)):\n",
        "  score_c7.append(merged_list_c7[i][3])\n",
        "\n",
        "score_c8 = []\n",
        "for i in range(0,len(merged_list_c8)):\n",
        "  score_c8.append(merged_list_c8[i][3])\n",
        "\n",
        "score_c9 = []\n",
        "for i in range(0,len(merged_list_c9)):\n",
        "  score_c9.append(merged_list_c9[i][3])\n",
        "\n",
        "score_c10 = []\n",
        "for i in range(0,len(merged_list_c10)):\n",
        "  score_c10.append(merged_list_c10[i][3])\n",
        "\n",
        "score_c32 = []\n",
        "for i in range(0,len(merged_list_c32)):\n",
        "  score_c32.append(merged_list_c32[i][3])\n",
        "\n",
        "score_c33 = []\n",
        "for i in range(0,len(merged_list_c33)):\n",
        "  score_c33.append(merged_list_c33[i][3])\n",
        "\n",
        "score_c61 = []\n",
        "for i in range(0,len(merged_list_c61)):\n",
        "  score_c61.append(merged_list_c61[i][3])\n",
        "\n",
        "score_c63 = []\n",
        "for i in range(0,len(merged_list_c63)):\n",
        "  score_c63.append(merged_list_c63[i][3])\n",
        "\n",
        "score_c71 = []\n",
        "for i in range(0,len(merged_list_c71)):\n",
        "  score_c71.append(merged_list_c71[i][3])\n",
        "\n",
        "score_c91 = []\n",
        "for i in range(0,len(merged_list_c91)):\n",
        "  score_c91.append(merged_list_c91[i][3])\n",
        "\n",
        "score_c94 = []\n",
        "for i in range(0,len(merged_list_c94)):\n",
        "  score_c94.append(merged_list_c94[i][3])\n",
        "\n",
        "score_c95 = []\n",
        "for i in range(0,len(merged_list_c95)):\n",
        "  score_c95.append(merged_list_c95[i][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shx362Na3aeB"
      },
      "outputs": [],
      "source": [
        "merged_list = []\n",
        "for i in range(len(hero)):\n",
        "  tup = (hero[i], pmid[i],cited[i],score_c3[i],score_c4[i],score_c5[i],score_c6[i],score_c7[i],score_c8[i],score_c9[i],score_c10[i],score_c32[i],score_c33[i],score_c61[i],score_c63[i],score_c71[i],score_c91[i],score_c94[i],score_c95[i])\n",
        "  merged_list.append(tup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m11SrFA13gKt"
      },
      "outputs": [],
      "source": [
        "merged_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWznZLEP3ikH"
      },
      "outputs": [],
      "source": [
        "all = pd.DataFrame(merged_list)  \n",
        "all.to_csv('merged_list_all.csv',index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#next step: 1. take the max of all the scores for each article, and use that as the final score\n",
        "#2. sort the articles based on the fianl score and calculate recall"
      ],
      "metadata": {
        "id": "3y1j5yVMl6Fb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "text_based_ensemble_LR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}