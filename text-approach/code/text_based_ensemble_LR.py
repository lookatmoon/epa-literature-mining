# -*- coding: utf-8 -*-
"""text_based_ensemble_LR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171aWxJwcZXl7HI2msazvET_l4oalPgY9
"""

import pandas as pd
import numpy as np

import re
import string
import csv

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

import sklearn
from sklearn.linear_model import Ridge,SGDRegressor,LinearRegression
from sklearn.metrics import recall_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer
from sklearn.linear_model import LogisticRegression

import matplotlib as mpl 
import matplotlib.pyplot as plt 

import scipy

"""#import the training sets"""

#import resulted training sets
c3_train = pd.read_csv('./c3.csv',engine='python',encoding = 'unicode_escape')
c4_train = pd.read_csv('./c4.csv',engine='python',encoding = 'unicode_escape')
c5_train = pd.read_csv('./c5.csv',engine='python',encoding = 'unicode_escape')
c6_train = pd.read_csv('./c6.csv',engine='python',encoding = 'unicode_escape')
c7_train = pd.read_csv('./c7.csv',engine='python',encoding = 'unicode_escape')
c8_train = pd.read_csv('./c8.csv',engine='python',encoding = 'unicode_escape')
c9_train = pd.read_csv('./c9.csv',engine='python',encoding = 'unicode_escape')
c10_train = pd.read_csv('./c10.csv',engine='python',encoding = 'unicode_escape')
c10_train = pd.read_csv('./c10_train_cleaned.csv',engine='python',encoding = 'unicode_escape')

c32_train = pd.read_csv('./c3.2.csv',engine='python',encoding = 'unicode_escape')
c33_train = pd.read_csv('./c3.3-3.8.csv',engine='python',encoding = 'unicode_escape')
c61_train = pd.read_csv('./c6.1-6.6.csv',engine='python',encoding = 'unicode_escape')
c63_train = pd.read_csv('./c6.3.csv',engine='python',encoding = 'unicode_escape')
c71_train = pd.read_csv('./c7.1-7.7.csv',engine='python',encoding = 'unicode_escape')
c91_train = pd.read_csv('./c9.1-9.6.csv',engine='python',encoding = 'unicode_escape')
c94_train = pd.read_csv('./c9.4.csv',engine='python',encoding = 'unicode_escape')
c95_train = pd.read_csv('./c9.5.csv',engine='python',encoding = 'unicode_escape')

test = pd.read_csv('./s2020.csv',encoding='utf-8',engine = 'python', error_bad_lines=False)
# test = pd.read_csv('./test.csv',engine='python',encoding='utf-8',error_bad_lines=False,index_col=False,names = ['InstanceId','HeroId','Part','Chapter','Section','Subsection','AnchorText','ContextParagraph','PMID','Title','Abstract','Cited'])

len(test)

"""#construct features"""

nltk.download('punkt')
nltk.download('stopwords')
STOP_WORDS = stopwords.words()

def cleaning(text):
    """
    Convert to lowercase.
    Rremove URL links, special characters and punctuation.
    Tokenize and remove stop words.
    """
    text = str(text).lower()
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('[’“”…]', '', text)

    # removing the stop-words
    text_tokens = word_tokenize(text)
    tokens_without_sw = [
        word for word in text_tokens if not word in STOP_WORDS]
    filtered_sentence = (" ").join(tokens_without_sw)
    text = filtered_sentence

    return text

def get_title_and_abstract(df):
    corpus = []
    for index, row in df.iterrows():
        title = row['TITLE']
        abstract = row['ABSTRACT']
        corpus.append( str(title) + ' ' + str(abstract))
    return corpus

vectorizer3 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c3_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer3_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c3_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer4 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c4_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer4_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c4_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer5 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c5_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer5_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c5_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer6 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c6_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer6_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c6_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer7 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c7_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer7_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c7_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer8 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c8_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer8_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c8_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer9 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c9_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer9_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c9_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer10 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c10_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer10_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c10_train['TITLE'].apply(cleaning).values.astype('U'))

vectorizer32 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c32_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer32_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c32_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer33 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c33_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer33_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c33_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer61 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c61_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer61_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c61_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer63 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c63_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer63_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c63_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer71 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c71_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer71_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c71_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer91 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c91_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer91_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c91_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer94 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c94_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer94_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c94_train['TITLE'].apply(cleaning).values.astype('U'))
vectorizer95 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c95_train['ABSTRACT'].apply(cleaning).values.astype('U'))
vectorizer95_2 = TfidfVectorizer(stop_words = None, ngram_range = (1,3)).fit(c95_train['TITLE'].apply(cleaning).values.astype('U'))

# construct sparse feature matrix
# params:
#     df: dataframe, with 'ABSTRACT' and 'TITLE' columns
#     vectorizer: sklearn text vectorizer, either TfidfVectorizer or Countvectorizer 
# return:
#     M: a sparse feature matrix that represents df's textual information (used by a predictive model)

def construct_feature_matrix(df, vectorizer, vectorizer2):
    abstract = df['ABSTRACT'].apply(lambda x: np.str_(x)).tolist()
    title = df['TITLE'].apply(lambda x: np.str_(x)).tolist()
  
    # here the dimensionality of X is len(df) x |V|
    X = vectorizer.transform(abstract)
    Y = vectorizer2.transform(title)
    Z = scipy.sparse.hstack([X,Y])

    return Z

train_Y_c3 = c3_train['CITED']
train_X_c3 = construct_feature_matrix(c3_train, vectorizer3, vectorizer3_2)
test_X_c3 = construct_feature_matrix(test, vectorizer3, vectorizer3_2)
print(train_X_c3.shape)
print(test_X_c3.shape)
train_Y_c4 = c4_train['CITED']
train_X_c4 = construct_feature_matrix(c4_train, vectorizer4, vectorizer4_2)
test_X_c4 = construct_feature_matrix(test, vectorizer4, vectorizer4_2)

train_Y_c5 = c5_train['CITED']
train_X_c5 = construct_feature_matrix(c5_train, vectorizer5, vectorizer5_2)
test_X_c5 = construct_feature_matrix(test, vectorizer5, vectorizer5_2)

train_Y_c6 = c6_train['CITED']
train_X_c6 = construct_feature_matrix(c6_train, vectorizer6, vectorizer6_2)
test_X_c6 = construct_feature_matrix(test, vectorizer6, vectorizer6_2)

train_Y_c7 = c7_train['CITED']
train_X_c7 = construct_feature_matrix(c7_train, vectorizer7, vectorizer7_2)
test_X_c7 = construct_feature_matrix(test, vectorizer7, vectorizer7_2)

train_Y_c8 = c8_train['CITED']
train_X_c8 = construct_feature_matrix(c8_train, vectorizer8, vectorizer8_2)
test_X_c8 = construct_feature_matrix(test, vectorizer8, vectorizer8_2)

train_Y_c9 = c9_train['CITED']
train_X_c9 = construct_feature_matrix(c9_train, vectorizer9, vectorizer9_2)
test_X_c9 = construct_feature_matrix(test, vectorizer9, vectorizer9_2)

train_Y_c10 = c10_train['CITED']
train_X_c10 = construct_feature_matrix(c10_train, vectorizer10, vectorizer10_2)
test_X_c10 = construct_feature_matrix(test, vectorizer10, vectorizer10_2)

train_Y_c32 = c32_train['CITED']
train_X_c32 = construct_feature_matrix(c32_train, vectorizer32, vectorizer32_2)
test_X_c32 = construct_feature_matrix(test, vectorizer32, vectorizer32_2)

train_Y_c33 = c33_train['CITED']
train_X_c33 = construct_feature_matrix(c33_train, vectorizer33, vectorizer33_2)
test_X_c33 = construct_feature_matrix(test, vectorizer33, vectorizer33_2)

train_Y_c61 = c61_train['CITED']
train_X_c61 = construct_feature_matrix(c61_train, vectorizer61, vectorizer61_2)
test_X_c61 = construct_feature_matrix(test, vectorizer61, vectorizer61_2)

train_Y_c63 = c63_train['CITED']
train_X_c63 = construct_feature_matrix(c63_train, vectorizer63, vectorizer63_2)
test_X_c63 = construct_feature_matrix(test, vectorizer63, vectorizer63_2)

train_Y_c71 = c71_train['CITED']
train_X_c71 = construct_feature_matrix(c71_train, vectorizer71, vectorizer71_2)
test_X_c71 = construct_feature_matrix(test, vectorizer71, vectorizer71_2)

train_Y_c91 = c91_train['CITED']
train_X_c91 = construct_feature_matrix(c91_train, vectorizer91, vectorizer91_2)
test_X_c91 = construct_feature_matrix(test, vectorizer91, vectorizer91_2)

train_Y_c94 = c94_train['CITED']
train_X_c94 = construct_feature_matrix(c94_train, vectorizer94, vectorizer94_2)
test_X_c94 = construct_feature_matrix(test, vectorizer94, vectorizer94_2)

train_Y_c95 = c95_train['CITED']
train_X_c95 = construct_feature_matrix(c95_train, vectorizer95, vectorizer95_2)
test_X_c95 = construct_feature_matrix(test, vectorizer95, vectorizer95_2)

"""#create helper functions"""

def merge(list1, list2, list3, list4):
    merged_list = []
    for i in range(max((len(list1), len(list2),len(list3),len(list4)))):
        while True:
            try:
                tup = (list1[i], list2[i], list3[i],list4[i])
            except IndexError:
                if len(list3) > len(list2):
                    list2.append('')
                    tup = (list1[i], list2[i],list3[i],list4[i])
                elif len(list3) < len(list2):
                    list3.append('')
                    tup = (list1[i], list2[i],list3[i],list4[i])
                continue
            merged_list.append(tup)
            break
    return merged_list

def calculate_recall(filepath,pmid_only,num):
  #filepath is a list
  pair = []
  filepath.sort(key=lambda y: y[3],reverse=True)
  if pmid_only == True:
    filepath=list(filter(lambda c: np.isnan(c[1]) == False, filepath))
  for i in range(1,len(filepath),num):
    tp = 0
    fn = 0
    for j in range(1,i):
      if filepath[j][2] == 1:
        tp += 1
    for k in range(i+1,len(filepath)):
      if filepath[k][2] == 1:
        fn += 1
    pair.append((i,tp/(tp+fn)))
  return pair

# def calculate_precision(filepath,pmid_only,num):
#   #filepath is a list
#   pair = []
#   if pmid_only == True:
#     filepath=list(filter(lambda c: np.isnan(c[1]) == False, filepath))
#   for i in range(1,len(filepath),num):
#     tp = 0
#     for j in range(1,i):
#       if filepath[j][2] == 1:
#         tp += 1
#     pair.append((i,tp))
#   result = []
#   result.append((0,0))
#   for i in range(0,len(pair)-1):
#     result.append((pair[i][0],(pair[i+1][1]-pair[i][1])/num))
  
#     # pair.append((i,tp/(i)))
#   return result

"""#start training"""

#C3
model_c3 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c3.fit(train_X_c3, train_Y_c3)
test_Y_hat_c3 = model_c3.predict_proba(test_X_c3)

prob_c3=[]
for i in range(0,len(test_Y_hat_c3)):
  prob_c3.append(test_Y_hat_c3[i][1])

#create list of tuples and sort
merged_list_c3 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c3)
merged_list_c3.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c3[0:10])

#C4
model_c4 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c4.fit(train_X_c4, train_Y_c4)
test_Y_hat_c4 = model_c4.predict_proba(test_X_c4)

prob_c4=[]
for i in range(0,len(test_Y_hat_c4)):
  prob_c4.append(test_Y_hat_c4[i][1])

#create list of tuples and sort
merged_list_c4 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c4)
merged_list_c4.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c4[0:10])

#C5
model_c5 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c5.fit(train_X_c5, train_Y_c5)
test_Y_hat_c5 = model_c5.predict_proba(test_X_c5)

prob_c5=[]
for i in range(0,len(test_Y_hat_c5)):
  prob_c5.append(test_Y_hat_c5[i][1])

#create list of tuples and sort
merged_list_c5 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c5)
merged_list_c5.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c5[0:10])

#C6
model_c6 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c6.fit(train_X_c6, train_Y_c6)
test_Y_hat_c6 = model_c6.predict_proba(test_X_c6)

prob_c6=[]
for i in range(0,len(test_Y_hat_c6)):
  prob_c6.append(test_Y_hat_c6[i][1])

#create list of tuples and sort
merged_list_c6 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c6)
merged_list_c6.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c6[0:10])

#C7
model_c7 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c7.fit(train_X_c7, train_Y_c7)
test_Y_hat_c7 = model_c7.predict_proba(test_X_c7)

prob_c7=[]
for i in range(0,len(test_Y_hat_c7)):
  prob_c7.append(test_Y_hat_c7[i][1])

#create list of tuples and sort
merged_list_c7 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c7)
merged_list_c7.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c7[0:10])

#C8
model_c8 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c8.fit(train_X_c8, train_Y_c8)
test_Y_hat_c8 = model_c8.predict_proba(test_X_c8)

prob_c8=[]
for i in range(0,len(test_Y_hat_c8)):
  prob_c8.append(test_Y_hat_c8[i][1])

#create list of tuples and sort
merged_list_c8 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c8)
merged_list_c8.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c8[0:10])

#C9
model_c9 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c9.fit(train_X_c9, train_Y_c9)
test_Y_hat_c9 = model_c9.predict_proba(test_X_c9)

prob_c9=[]
for i in range(0,len(test_Y_hat_c9)):
  prob_c9.append(test_Y_hat_c9[i][1])

#create list of tuples and sort
merged_list_c9 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c9)
merged_list_c9.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c9[0:10])

#C10
model_c10 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c10.fit(train_X_c10, train_Y_c10)
test_Y_hat_c10 = model_c10.predict_proba(test_X_c10)

prob_c10=[]
for i in range(0,len(test_Y_hat_c10)):
  prob_c10.append(test_Y_hat_c10[i][1])

#create list of tuples and sort
merged_list_c10 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c10)
merged_list_c10.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c10[0:10])

#C3.2
model_c32 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c32.fit(train_X_c32, train_Y_c32)
test_Y_hat_c32 = model_c32.predict_proba(test_X_c32)

prob_c32=[]
for i in range(0,len(test_Y_hat_c32)):
  prob_c32.append(test_Y_hat_c32[i][1])

#create list of tuples and sort
merged_list_c32 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c32)
merged_list_c32.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c32[0:10])

#C3.3
model_c33 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c33.fit(train_X_c33, train_Y_c33)
test_Y_hat_c33 = model_c33.predict_proba(test_X_c33)

prob_c33=[]
for i in range(0,len(test_Y_hat_c33)):
  prob_c33.append(test_Y_hat_c33[i][1])

#create list of tuples and sort
merged_list_c33 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c33)
merged_list_c33.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c33[0:10])

#C6.1
model_c61 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c61.fit(train_X_c61, train_Y_c61)
test_Y_hat_c61 = model_c61.predict_proba(test_X_c61)

prob_c61=[]
for i in range(0,len(test_Y_hat_c61)):
  prob_c61.append(test_Y_hat_c61[i][1])

#create list of tuples and sort
merged_list_c61 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c61)
merged_list_c61.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c61[0:10])

#C6.3
model_c63 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c63.fit(train_X_c63, train_Y_c63)
test_Y_hat_c63 = model_c63.predict_proba(test_X_c63)

prob_c63=[]
for i in range(0,len(test_Y_hat_c63)):
  prob_c63.append(test_Y_hat_c63[i][1])

#create list of tuples and sort
merged_list_c63 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c63)
merged_list_c63.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c63[0:10])

#C7.1
model_c71 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c71.fit(train_X_c71, train_Y_c71)
test_Y_hat_c71 = model_c71.predict_proba(test_X_c71)

prob_c71=[]
for i in range(0,len(test_Y_hat_c71)):
  prob_c71.append(test_Y_hat_c71[i][1])

#create list of tuples and sort
merged_list_c71 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c71)
merged_list_c71.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c71[0:10])

#C9.1
model_c91 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c91.fit(train_X_c91, train_Y_c91)
test_Y_hat_c91 = model_c91.predict_proba(test_X_c91)

prob_c91=[]
for i in range(0,len(test_Y_hat_c91)):
  prob_c91.append(test_Y_hat_c91[i][1])

#create list of tuples and sort
merged_list_c91 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c91)
merged_list_c91.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c91[0:10])

#C9.4
model_c94 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c94.fit(train_X_c94, train_Y_c94)
test_Y_hat_c94 = model_c94.predict_proba(test_X_c94)

prob_c94=[]
for i in range(0,len(test_Y_hat_c94)):
  prob_c94.append(test_Y_hat_c94[i][1])

#create list of tuples and sort
merged_list_c94 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c94)
merged_list_c94.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c94[0:10])

#C9.5
model_c95 = LogisticRegression(penalty='l2',C=1.0,class_weight='balanced')
model_c95.fit(train_X_c95, train_Y_c95)
test_Y_hat_c95 = model_c95.predict_proba(test_X_c95)

prob_c95=[]
for i in range(0,len(test_Y_hat_c95)):
  prob_c95.append(test_Y_hat_c95[i][1])

#create list of tuples and sort
merged_list_c95 = merge(test['REFERENCE_ID'],test['PMID'],test['CITED'],prob_c95)
merged_list_c95.sort(key=lambda y: y[3],reverse=True)
print(merged_list_c95[0:10])

merged_list_c3.sort(key=lambda y: y[0],reverse=False)
merged_list_c4.sort(key=lambda y: y[0],reverse=False)
merged_list_c5.sort(key=lambda y: y[0],reverse=False)
merged_list_c6.sort(key=lambda y: y[0],reverse=False)
merged_list_c7.sort(key=lambda y: y[0],reverse=False)
merged_list_c8.sort(key=lambda y: y[0],reverse=False)
merged_list_c9.sort(key=lambda y: y[0],reverse=False)
merged_list_c10.sort(key=lambda y: y[0],reverse=False)

merged_list_c32.sort(key=lambda y: y[0],reverse=False)
merged_list_c33.sort(key=lambda y: y[0],reverse=False)
merged_list_c61.sort(key=lambda y: y[0],reverse=False)
merged_list_c63.sort(key=lambda y: y[0],reverse=False)
merged_list_c71.sort(key=lambda y: y[0],reverse=False)
merged_list_c91.sort(key=lambda y: y[0],reverse=False)
merged_list_c94.sort(key=lambda y: y[0],reverse=False)
merged_list_c95.sort(key=lambda y: y[0],reverse=False)

hero = []
for i in range(0,len(merged_list_c3)):
  hero.append(merged_list_c4[i][0])

pmid = []
for i in range(0,len(merged_list_c3)):
  pmid.append(merged_list_c4[i][1])
cited = []
for i in range(0,len(merged_list_c3)):
  cited.append(merged_list_c4[i][2])

score_c3 = []
for i in range(0,len(merged_list_c3)):
  score_c3.append(merged_list_c3[i][3])

score_c4 = []
for i in range(0,len(merged_list_c4)):
  score_c4.append(merged_list_c4[i][3])

score_c5 = []
for i in range(0,len(merged_list_c5)):
  score_c5.append(merged_list_c5[i][3])

score_c6 = []
for i in range(0,len(merged_list_c6)):
  score_c6.append(merged_list_c6[i][3])

score_c7 = []
for i in range(0,len(merged_list_c7)):
  score_c7.append(merged_list_c7[i][3])

score_c8 = []
for i in range(0,len(merged_list_c8)):
  score_c8.append(merged_list_c8[i][3])

score_c9 = []
for i in range(0,len(merged_list_c9)):
  score_c9.append(merged_list_c9[i][3])

score_c10 = []
for i in range(0,len(merged_list_c10)):
  score_c10.append(merged_list_c10[i][3])

score_c32 = []
for i in range(0,len(merged_list_c32)):
  score_c32.append(merged_list_c32[i][3])

score_c33 = []
for i in range(0,len(merged_list_c33)):
  score_c33.append(merged_list_c33[i][3])

score_c61 = []
for i in range(0,len(merged_list_c61)):
  score_c61.append(merged_list_c61[i][3])

score_c63 = []
for i in range(0,len(merged_list_c63)):
  score_c63.append(merged_list_c63[i][3])

score_c71 = []
for i in range(0,len(merged_list_c71)):
  score_c71.append(merged_list_c71[i][3])

score_c91 = []
for i in range(0,len(merged_list_c91)):
  score_c91.append(merged_list_c91[i][3])

score_c94 = []
for i in range(0,len(merged_list_c94)):
  score_c94.append(merged_list_c94[i][3])

score_c95 = []
for i in range(0,len(merged_list_c95)):
  score_c95.append(merged_list_c95[i][3])

merged_list = []
for i in range(len(hero)):
  tup = (hero[i], pmid[i],cited[i],score_c3[i],score_c4[i],score_c5[i],score_c6[i],score_c7[i],score_c8[i],score_c9[i],score_c10[i],score_c32[i],score_c33[i],score_c61[i],score_c63[i],score_c71[i],score_c91[i],score_c94[i],score_c95[i])
  merged_list.append(tup)

merged_list

all = pd.DataFrame(merged_list)  
all.to_csv('merged_list_all.csv',index = False)

#next step: 1. take the max of all the scores for each article, and use that as the final score
#2. sort the articles based on the final score and calculate recall
#3. format the results and use only the four columns ['REFERENCE_ID', 'PMID', 'Label', 'Score']
