# -*- coding: utf-8 -*-
"""Data Exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16s1-z2h0cWu8ha1Gi71ikCwfPDioyFEN
"""

import re
import string
import nltk

import pandas as pd
import numpy as np

from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from gensim import corpora
import gensim

"""#File import"""

#the file s2020 is available here: https://drive.google.com/drive/folders/1wkQN4_byG8uVf6AduA8nsC3QHN5ZOXH6?usp=sharing
filepath = './c2013.csv'
c2013 = pd.read_csv(filepath,engine='python')
filepath2 = './c2020.csv'
c2020 = pd.read_csv(filepath2,engine='python')
filepath3 = './s2013.csv'
s2013 = pd.read_csv(filepath3,engine='python')
filepath4 = './s2020.csv'
s2020 = pd.read_csv(filepath4,engine='python',error_bad_lines=False)

"""#Data Cleaning and Extraction



"""

nltk.download('punkt')
nltk.download('stopwords')

STOP_WORDS = stopwords.words()

def cleaning(text):
    """
    Convert to lowercase.
    Rremove URL links, special characters and punctuation.
    Tokenize and remove stop words.
    """
    text = str(text).lower()
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('[’“”…]', '', text)

    # removing the stop-words
    text_tokens = word_tokenize(text)
    tokens_without_sw = [
        word for word in text_tokens if not word in STOP_WORDS]
    filtered_sentence = (" ").join(tokens_without_sw)
    text = filtered_sentence

    return text
    
# df = df['TITLE'].apply(cleaning)

# counting unigrams and bigrams
def count_unigrams(df):
  word_count = Counter(" ".join(df).split()).most_common(15)
  word_frequency = pd.DataFrame(word_count, columns = ['Word', 'Frequency'])
  return word_frequency

def count_bigrams(df):
  words = nltk.tokenize.word_tokenize(df.to_string())
  bigram = list(nltk.bigrams(words))
  frequency = {}
  for item in bigram:
    if item in frequency:
        frequency[item] += 1
    else:
        frequency[item] = 1
  word_dist = nltk.FreqDist([' '.join(x) for x in bigram])
  bigram_frequency = pd.DataFrame(word_dist.most_common(15),columns=['Word', 'Frequency'])
  return bigram_frequency

pd.options.display.max_colwidth = 120
#choose the specific function, file, and column name to count unigrams/bigrams within your desired dataset
#E.g: here, we count the most frequency bigrams appearing in the abstratcs of articles cited in 2013
count_bigrams(c2013['ABSTRACT'].apply(cleaning))

"""#Data Exploration"""

# topic modeling
nltk.download('stopwords')
en_stop = set(nltk.corpus.stopwords.words('english'))

def prepare_text_for_lda(text):
    # convert all words into lower case, split by white space
    tokens = str(text).strip().lower().split()
    
    #  remove words with 1 or 2 letters (small words, punctuation)
    tokens = [token for token in tokens if len(token) > 2]
    tokens = [token for token in tokens if token not in en_stop]
    return tokens

# take out title texts in a list
c2013_titles = []
for index, row in c2013.iterrows():
    title_text = c2013['ABSTRACT']
    c2013_titles.append( title_text )
    
print(c2013_titles[0])

s2013_titles = []
for index, row in s2013.iterrows():
    title_text = row['TITLE']
    s2013_titles.append( title_text )
    
print(s2013_titles[0])

c2020_titles = []
for index, row in c2020.iterrows():
    title_text = row['TITLE']
    c2020_titles.append( title_text )
    
print(c2020_titles[0])

s2020_titles = []
for index, row in s2020.iterrows():
    title_text = row['TITLE']
    s2020_titles.append( title_text )
    
print(s2020_titles[0])

# take out abstract texts in a list
c2013_abstracts = []
for index, row in c2013.iterrows():
    abstract_text = row['ABSTRACT']
    c2013_abstracts.append( abstract_text )

print (c2013_abstracts[0])

s2013_abstracts = []
for index, row in s2013.iterrows():
    abstract_text = row['ABSTRACT']
    s2013_abstracts.append( abstract_text )

print (s2013_abstracts[0])

c2020_abstracts = []
for index, row in c2020.iterrows():
    abstract_text = row['ABSTRACT']
    c2020_abstracts.append( abstract_text )

print (c2020_abstracts[0])

s2020_abstracts = []
for index, row in s2020.iterrows():
    abstract_text = row['ABSTRACT']
    s2020_abstracts.append( abstract_text )

print (s2020_abstracts[0])

#Use desired lists from above for topic modeling
#E.g: topic modeling on the abstracts of articles cited in 2020 
c2020_text_abstract = []
for abstract in c2020_abstracts:
    abstract = prepare_text_for_lda(abstract)
    c2020_text_abstract.append(abstract)
    
c2020_abstract_dictionary = corpora.Dictionary(c2020_text_abstract)
c2020_abstract_corpus = [c2020_abstract_dictionary.doc2bow(text) for text in c2020_text_abstract]

# train latent Dirichlet topic model
NUM_TOPICS = 20
ldamodel = gensim.models.ldamodel.LdaModel(c2020_abstract_corpus, num_topics = NUM_TOPICS, id2word=c2020_abstract_dictionary, passes=15)

topics = ldamodel.print_topics(num_words=6)
for topic in topics:
    print(topic)